# -*- coding: utf-8 -*-
"""Document Clustering Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YMNe7vdUTEdv6FnYxF8rgYBVSPOxszp3
"""

#Task 2. Document Clustering
import matplotlib.pyplot as plt
import pandas as pd

### Step 1: Load File in Dataset
file = open('RSSFeed.txt', encoding='utf8')
dataset = file.read().split("\n")
print(dataset)
print(file)

# Data Pre-processing 
import nltk
import string
nltk.download("punkt")
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
ps = PorterStemmer()

def doc_preprocess(text):
  nltk.download("stopwords")
  from nltk.corpus import stopwords
  sw = stopwords.words('english')
  sw += ("?",".",":",",",")","(","Sports","Politics","Health")
  filtered_docs = []
  for doc in text:
    tokens = word_tokenize(doc)
    tmp = ""
    for word in tokens:
        if word not in sw:
          # remove punctuations
          trans = str.maketrans('', '', string.punctuation)
          word = word.translate(trans)
          # Lowercase the document
          word = word.lower()
          tmp += ps.stem(word) + " "
    filtered_docs.append(tmp)
  return filtered_docs

# Final clean_doc for Vectorization
clean_doc = doc_preprocess(dataset)
print(clean_doc)

# Convert Filtered Documnets into Vectors
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(clean_doc)
print(X.todense())

## Using Elbow Methode to Find Optimal Number of Clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1 , 11):
  kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

## Build a Clustering Model Using K-Mean Algorithm
from sklearn.cluster import KMeans
K = 3
kmodel = KMeans(n_clusters = K, init = 'k-means++',random_state = 42)
y_pred = kmodel.fit_predict(X)
print(y_pred)

# 0:Sports 2:Politics 1:Health 
# Evaluaction of Model
import numpy as np
from sklearn.metrics import precision_recall_fscore_support as prfs
from sklearn.metrics.cluster import rand_score

if __name__ == "__main__":
    preds  = [0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]
    labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2]
    print("preds:", preds)
    print("labels:", labels)
    print()
    pred_arr = np.array(preds)
    label_arr = np.array(labels)
    match_arr = pred_arr == label_arr
    print("matches:", match_arr)
    print()
    RI = rand_score(pred_arr,label_arr)
    print('Rand Index: ', RI)
    print()

test_doc = [" Pick up ten and it is two years without international cricket.",
            "I have already said that the players we have, many of them have been playing for their country and are experienced players.",
            " Crypto doesnâ€™t give a shit what you do with it. For my purpose, and for the purpose of the customer.",
            " By changing our habits and making choices that have less harmful effects on the environment, we have the power to confront the climate challenge and build a more sustainable world.",
            "The Agreement entered into force less than a year later. In the agreement, all countries agreed to work to limit global temperature rise to well below 2 degrees Celsius, and given the grave risks, to strive for 1.5 degrees Celsius.",
            "You've got emergency services personnel that are struggling to put their heating on and food on the table and you just think: 'that is not what we expect or should be giving to the people that are helping our country."]

filtered_test_docs = doc_preprocess(test_doc)
print(filtered_test_docs)

vect = TfidfVectorizer(tokenizer=doc_preprocess)


vectorized_text = vect.fit_transform(test_doc)

kmeans = KMeans(n_clusters=1).fit(vectorized_text)

 # now predicting the cluster for given dataset

# Cluster Prediction for each document
Y = vectorizer.transform([filtered_test_docs[0]])
prediction = kmodel.predict(Y)
print(prediction)

Y1 = vectorizer.transform([filtered_test_docs[1]])
prediction1 = kmodel.predict(Y1)
print(prediction1)

Y2 = vectorizer.transform([filtered_test_docs[2]])
prediction2 = kmodel.predict(Y2)
print(prediction2)

Y3= vectorizer.transform([filtered_test_docs[3]])
prediction3 = kmodel.predict(Y3)
print(prediction3)

Y4= vectorizer.transform([filtered_test_docs[4]])
prediction4 = kmodel.predict(Y4)
print(prediction4)

Y5= vectorizer.transform([filtered_test_docs[5]])
prediction5 = kmodel.predict(Y5)
print(prediction5)

Y
print(Y)

Y1
print (Y1)

Y2
print (Y2)

Y3
print (Y3)

Y4
print (Y4)

Y5
print (Y5)