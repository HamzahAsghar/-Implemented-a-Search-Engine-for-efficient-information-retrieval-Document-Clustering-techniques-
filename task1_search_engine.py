# -*- coding: utf-8 -*-
"""Task1- Search Engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hD9A89CiPFuZqm4d1uNuMRDwVP8YKfKa
"""

from urllib.parse import urlencode

import numpy as np
import pandas as pd

## Get All Publication Links
import requests
from bs4 import BeautifulSoup

# Get All Links form First Page
text = requests.get('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/').text
soup = BeautifulSoup(text, "html.parser")
links = []
all_papers = soup.find_all("a", class_='link')
for link in all_papers:
    links.append(link.get("href"))
sturl = links.pop(0)
sturl = links.pop(0)

# Get All Links form Remaining Pages (2 to 10) 
for page in range(1,11):
    req = requests.get('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/?page='+ str(page)).text
    soup = BeautifulSoup(req, 'html.parser')
    allinks = soup.find_all("a", class_='link')
    for link in allinks:
      links.append(link.get("href"))

# Extract Only Publication Paper's Links
finalpapers = []
for i in range(0, len(links)):
  if links[i].find('https://pureportal.coventry.ac.uk/en/publications') != -1:
     finalpapers.append(links[i])

from bs4 import BeautifulSoup
import requests

url = "https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/?page="
page = requests.get(url)
print(page.text)

!pip install feedparser
import feedparser
NewsFeed = feedparser.parse("https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/?page=/edition.rss")
entry = NewsFeed.entries[1]

for entry in NewsFeed.entries:
    print(entry.title)
    print(entry.link)
    print("--------------------------------------")

## For Each Published Paper Extract Details
paperDeatils = []
csvDetails = []
#test = finalpapers[:100 ]
totalAuthors = []
header = ["Paper_Links", "Paper_Names", "Publication_Date", "Author_Names","Author_Links","Abstract"]
k = 0

for i in finalpapers:
  r = requests.get(i)
  soup = BeautifulSoup(r.content, 'html.parser')
  allauthornm = ' '
  allauthorlk = ' '
  chksls = ' '
  abstract1 = 'NA'
  papernm = soup.find('h1')
  paperauther = soup.find_all("a", class_= 'link person')
  paperdate = soup.find("span", class_= 'date')
  slslink = soup.find_all("li", class_ = 'researchgroup')
  abstract = soup.find("div", class_ = 'textblock')

  for l in slslink:
    chksls = chksls + l.string
  if chksls.find('Research Centre for Computational Science and Mathematical Modelling') != -1:
     if abstract is not None:
      abstract1 = abstract.text
     else:
      abstract1 = 'NA'

     for j in paperauther:
       allauthornm = allauthornm + (' ' + j.string + ' ')
       allauthorlk = allauthorlk + (j['href'] +' ')

     if allauthornm != ' ':
       totalAuthors.append(j['href'] +' ')
       csvDetails.append({
           "Paper_Links" : finalpapers[k],
           "Paper_Names" : papernm.string,
           "Publication_Date" : paperdate.string, 
           "Author_Names" : allauthornm,
           "Author_Links" : allauthorlk,
           "Abstract" : abstract1
        })
       paperDeatils.append(papernm.string +' ' + finalpapers[k] + ' ' + paperdate.string + ' ' 
                           + allauthornm + ',' + allauthorlk)
  k = k+1

print(paperDeatils)
print("\n\n")
print(totalAuthors) 
print("\n\n")
print(len(totalAuthors))
print("\n\n")
print(csvDetails)
print("\n\n")
print(len(paperDeatils))

import csv
# open the file in the write mode
fo = open('Publications.csv', "w")
# create the csv writer
writer = csv.writer(fo)
with open('Publications.csv', 'w', newline='') as output_file:
  dict_writer = csv.DictWriter(output_file, fieldnames = header)
  dict_writer.writeheader()
  dict_writer.writerows(csvDetails)

## 1. Get Total Number of Authors
uniqueList = []
for author in totalAuthors:
    exist = False
    for x in uniqueList:
        if x == author:
            exist = True
            break
    if not exist :
        uniqueList.append(author)
print(uniqueList)
print(len(uniqueList))

# create a paper_index (with Author name, Paper name, Abstract )
df = pd.read_csv('Publications.csv')
paper_index = df.Author_Names + df.Paper_Names + df.Abstract
print(paper_index)

# Data Pre-processing
import nltk
import string
nltk.download("punkt")
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
ps = PorterStemmer()

def doc_preprocess(text):
  nltk.download("stopwords")
  from nltk.corpus import stopwords
  sw = stopwords.words('english')
  filtered_docs = []
  for doc in text:
    tokens = word_tokenize(str(doc))
    tmp = ""
    for word in tokens:
        if word not in sw:
          # remove punctuations
          trans = str.maketrans('', '', string.punctuation)
          word = word.translate(trans)
          # Lowercase the document
          word = word.lower()
          tmp += ps.stem(word) + " "
    filtered_docs.append(tmp)
  return filtered_docs

# Final clean_doc for indexing
clean_doc = doc_preprocess(paper_index)
print(clean_doc)

# Implementation of Inverted_Index
def generate_inverted_index(data: list):
  inverted_index = {}

  for i, doc in enumerate(data):
    for term in doc.split():
      # Index will add new word only when that is not exist in Inverted Index
      if (term in inverted_index) and (doc not in inverted_index[term]):
        inverted_index[term].append(i)
      else:
        inverted_index[term] = [i]
  return inverted_index 

import csv
inverted_index = generate_inverted_index(clean_doc)
print(inverted_index)
with open('Inverted_Index.csv', 'w', newline='') as f:
  writer = csv.writer(f)
  for k, v in inverted_index.items():
    writer.writerow([k,v])

### Create Term-Document Matrix with TF-IDF weighting
from sklearn.feature_extraction.text import TfidfVectorizer
# Instantiate a TfidfVectorizer object
vectorizer = TfidfVectorizer()
# It fits the data and transform it as a vector
X = vectorizer.fit_transform(clean_doc)
# Convert the X as transposed matrix
X = X.T.toarray()
# Create a DataFrame and set the vocabulary as the index
df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())
print(df)

def getIndexed_doc(q):
  indexeddoc = []
  words = q[0].split(' ')
  for term in words:
    #print(term)
    for key in inverted_index.keys():
      if term == key:
        indexeddoc.append(inverted_index[key])
  return indexeddoc

from datetime import datetime
def getResults(query,df):
  print("Query:", query)
  print("Following are the items with heighest Cosine Similarity: ")
  # Convert the query become a vector
  start = datetime.now()
  q = [query]
  query_clean = " "
  query_clean = doc_preprocess(q)
  print(query_clean)
  matchingDoc = getIndexed_doc(query_clean)
  #print(matchingDoc)
  matchdoc = set(matchingDoc[0])
  l = len(matchdoc)
  print(matchdoc)
  finaldoclist = []
  finaldoclist = list(matchdoc)
  q_vec = vectorizer.transform(query_clean).toarray().reshape(df.shape[0],)
  sim = {}
  
    # Calculate the similarity
  for i in (matchdoc):
    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)
  
  # Sort the values 
  sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)
  end = datetime.now()
  etime = end - start
  print('About ',l, ' Results')
  print("Execution Time:", etime.total_seconds(),"Seconds")
  # Print the articles and their similarity values
  for k, v in sim_sorted:
    if v != 0.0:
      print()
      print("Cosine Similaritas:", v)
      print(paperDeatils[k])
      print()

# Add The Query
try:
 q1 = input("Please Enter Query:") 
# Call the function
 getResults(q1, df)
except:
  print("No Result Found")